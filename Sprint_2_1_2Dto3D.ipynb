{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint_2.1_2Dto3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importante:\n",
        "* Es ideal utilizar imagenes en alta definición\n",
        "* Si quiere ejecutar el codigo es necesario que el entorno de ejecución sea la GPU"
      ],
      "metadata": {
        "id": "cIjs-oMOQWe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalamos mediapipe para realizar la segmentación de imagenes\n",
        "!pip install mediapipe\n",
        "#Clonamos el repositorio de \"PIFUHD\"\n",
        "!git clone https://github.com/facebookresearch/pifuhd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z55b7VIqmD-s",
        "outputId": "f35aa88b-f668-4347-a7ae-21d262616007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.7 MB 130 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (3.10.0.2)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.9.1\n",
            "Cloning into 'pifuhd'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Total 213 (delta 0), reused 0 (delta 0), pack-reused 213\u001b[K\n",
            "Receiving objects: 100% (213/213), 402.72 KiB | 3.60 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#Clonamos el repositorio de \"lightweight\" que nos ayudará a obtener el esqueleto de nuestra imagen\n",
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git\n",
        "#Nos dirigimos dentro del directorio \"lightweight\"\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "#Descargamos el modelo pre-entrenado de \"lightweight\"\n",
        "!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
        "\n",
        "#Ejecutamos el siguiente codigo que realiza un reshape de la resolución de la imagen que le pasaremos\n",
        "#ya que alparecer 250 es la maxima resolución con la que se peude entrenar en Google Colab \n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "import demo\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "            \n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "              pmin = valid_keypoints.min(0)\n",
        "              pmax = valid_keypoints.max(0)\n",
        "\n",
        "              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n",
        "              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "              # if leg is missing, use pelvis to get cropping\n",
        "              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n",
        "              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n",
        "              center[1] += int(0.05*radius)\n",
        "            else:\n",
        "              center = np.array([img.shape[1]//2,img.shape[0]//2])\n",
        "              radius = max(img.shape[1]//2,img.shape[0]//2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2*radius, 2*radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')\n",
        "\n",
        "#Nos dirigimos dentro del directorio \"pifuhd\"\n",
        "%cd /content/pifuhd/\n",
        "#Descargamos el modelo pre-entrenado de conversiones 2D a 3D\n",
        "!sh ./scripts/download_trained_model.sh"
      ],
      "metadata": {
        "id": "WLuePNB2LnKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2532958c-86f4-4db7-fd6f-9f167a7c1259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'lightweight-human-pose-estimation.pytorch'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 120 (delta 1), reused 0 (delta 0), pack-reused 115\u001b[K\n",
            "Receiving objects: 100% (120/120), 227.79 KiB | 544.00 KiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n",
            "/content/lightweight-human-pose-estimation.pytorch\n",
            "--2022-03-31 02:03:17--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
            "Resolving download.01.org (download.01.org)... 23.194.168.145, 2600:1407:3c00:1480::4b21, 2600:1407:3c00:1487::4b21\n",
            "Connecting to download.01.org (download.01.org)|23.194.168.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87959810 (84M) [application/octet-stream]\n",
            "Saving to: ‘checkpoint_iter_370000.pth’\n",
            "\n",
            "checkpoint_iter_370 100%[===================>]  83.88M   158MB/s    in 0.5s    \n",
            "\n",
            "2022-03-31 02:03:18 (158 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n",
            "\n",
            "/content/pifuhd\n",
            "+ mkdir -p checkpoints\n",
            "+ cd checkpoints\n",
            "+ wget https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt pifuhd.pt\n",
            "--2022-03-31 02:03:27--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1548375177 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘pifuhd.pt’\n",
            "\n",
            "pifuhd.pt           100%[===================>]   1.44G  58.1MB/s    in 21s     \n",
            "\n",
            "2022-03-31 02:03:48 (69.1 MB/s) - ‘pifuhd.pt’ saved [1548375177/1548375177]\n",
            "\n",
            "--2022-03-31 02:03:49--  http://pifuhd.pt/\n",
            "Resolving pifuhd.pt (pifuhd.pt)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘pifuhd.pt’\n",
            "FINISHED --2022-03-31 02:03:49--\n",
            "Total wall clock time: 22s\n",
            "Downloaded: 1 files, 1.4G in 21s (69.1 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pasamos al script de transformación 2D a 3D"
      ],
      "metadata": {
        "id": "tkYKa_hJeJvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nos dirigimos al directorio \"content\"\n",
        "%cd /content/\n",
        "\n",
        "'''\n",
        "¡¡ATENCION!! \n",
        "  *El programa presenta mejores resultados con imagenes en alta resolución\n",
        "  *Para obtener siluetas correctas es recomendable recogerse el cabello largo antes de tomarse la foto y así dejar ver la medida de los hombros\n",
        "\n",
        "1.Utilizar versiones anteriores a Python 3.10 y mayores iguales que Python 3.7\n",
        "2.Instalar ndimage.whl segun las caracteristicas de su OS del siguiente enlace https://www.lfd.uci.edu/~gohlke/pythonlibs/#ndimage \n",
        "3.De la siguiente manera podrá instalar en el terminal el archivo .whl --- $ pip install \"nombre del archivo\".whl\n",
        "4.Finalmente, instale las liberías correspondientes numpy,OpenCV,math,mediapipe y scipy\n",
        "'''\n",
        "#Importamos las librerías necesarias\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import os\n",
        "from scipy.ndimage.morphology import binary_erosion\n",
        "\n",
        "#Obtenemos la clase selfie_segmentation, la cual nos ayudará a realizar la segmentación\n",
        "segmentation=mp.solutions.selfie_segmentation\n",
        "\n",
        "\n",
        "#Incializamos un contexto \"with\" y utilizamos los valores configurables de SelfieSegmentation\n",
        "#Tenemos los valores de 0 ó 1 como elección, en este caso escogeremos 1\n",
        "#ya que nos permite de un modelo de rango completo mejor figuras dentro de los 5 metros\n",
        "with segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n",
        "  #Inicializamos la imagen con OpenCV\n",
        "  '''\n",
        "  ACAAAAA PONEMOS LA RUTA DE LA IMAGEN QUE QUEREMOS SEGEMENTAR Y LUEGO CONVERTIRLO A 3D!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "  '''\n",
        "  im=cv2.imread(\"/content/sample_data/prueba_2.png\")\n",
        "  im_rgb = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  #Convertimos nuestra imagen en formato BGR a RGB y procesamos con  MediaPipe Selfie Segmentation \n",
        "  im_result=selfie_segmentation.process(im_rgb)\n",
        "\n",
        "    #A la imagen resultante obtendremos su UMBRAL, donde si los valores dentro de la imagen son mayores \n",
        "  #que 0.75 se visualizaran en blanco (255) y los menores en negro(0), para esto tambien le \n",
        "  #agregamos que queremos hacer una binarización binaria invertida, lo que resultará que tengamos\n",
        "  #un fondo de color blanco\n",
        "  _,tr_binary=cv2.threshold(im_result.segmentation_mask,0.75,255,cv2.THRESH_BINARY)\n",
        "  \n",
        "  #Una vez obtenida el umbral de la imagen, pasamos a realizarle una erosión donde rellenaremos los espacios faltantes en la imagen\n",
        "  #El rellenador tendrá un tamaño de 1x1 y se realizaran 20 iteraciones donde buscará las partes por rellenar en la imagen\n",
        "  eroded = binary_erosion(tr_binary, structure=np.ones((1, 1)), iterations=20)\n",
        "  #Obtendremos una matriz con valores True y False\n",
        "  #De la siguiente manera haremos el cambio de tipo de dato a float32\n",
        "  image=np.uint8(eroded)\n",
        "  #Despues de convertir la matriz de la imagen a valores tipo \"uint8\"\n",
        "  #Cambiamos los valores diferentes a cero por 255 \n",
        "  image[image!=0]=255\n",
        "  #Realizamos un filtro medianBlur para suavizar la imagen\n",
        "  tr_binary = cv2.medianBlur(image, 5)\n",
        " \n",
        "  #Invertimos el threshold\n",
        "  tr_binary_inv=cv2.bitwise_not(tr_binary)\n",
        "\n",
        "  #Creamos una matriz con las mismaas dimensiones de nuestra imagen original\n",
        "  bg_image = np.ones(im.shape,dtype=np.uint8)\n",
        "  #Ah esa matriz le daremos un color de fondo\n",
        "  bg_image[:]=(255,255,255)\n",
        "  #Haremos un operacion AND (esta está por demás ya que lo hacemos con las dos imagenes)\n",
        "  #Lo que nos ayuda de esta función es la posibilidad de poner una mascara\n",
        "  #La forma de esta mascara será dada a la imagen que estamos colocando en la operación\n",
        "  #Este serpa nuestro fondo\n",
        "  bg = cv2.bitwise_and(bg_image,bg_image, mask=tr_binary_inv)\n",
        "\n",
        "  #Haremos lo mismo aca pero con nuestra imagen de la persona y le pondremos la forma que obtuvimos\n",
        "  #apartir de la segentación\n",
        "  #Este será nuestra persona\n",
        "  fg = cv2.bitwise_and(im, im, mask=tr_binary)\n",
        "\n",
        "  #Sumamos las dos imagenes(fondo y la imagen segmentada)\n",
        "  image_output=cv2.add(bg,fg)\n",
        "  #Obtenemos la ruta donde se encuentra nuestra imagen \n",
        "  image_path=\"/content/pifuhd/sample_images/test.png\"\n",
        "  #Guardamos en la ruta de la carpeta pifuhd\n",
        "  cv2.imwrite(image_path,image_output)\n",
        "\n",
        "#Obtnemos la ruta del directorio de la imagen \n",
        "image_dir = os.path.dirname(image_path)\n",
        "#Obtenemos el nombre del archivo imagen\n",
        "file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "#Ponemos la ruta deseada de nuestro objeto\n",
        "obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n",
        "\n",
        "#Nos dirigimos al directorio \"content/lightweight-human-pose-estimation.pytorch\"\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "\n",
        "#Obtenemos el esqueleto de nuestra imagen\n",
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "\n",
        "#Obtenemos el esqueleto de nuestra imagen\n",
        "get_rect(net.cuda(), [image_path], 512)\n",
        "\n",
        "#Nos dirigimos al directorio \"/content/pifuhd/\"\"\n",
        "%cd /content/pifuhd/\n",
        "#Ejecutamos el script de apps.simple_test con una resulución 256\n",
        "#obteniendo datos de la ruta del directorio reservado en la variable image_dir\n",
        "!python -m apps.simple_test -r 256 --use_rect -i $image_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y0STyygMsTT",
        "outputId": "00f2a992-1263-4c16-d705-11ea5e35984a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/lightweight-human-pose-estimation.pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encuentre su resultado"
      ],
      "metadata": {
        "id": "h97SLYTdgetd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#La ruta del resultado es /content/pifuhd/results/pifuhd_final/recon/"
      ],
      "metadata": {
        "id": "Ec-6cI5OgeF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}